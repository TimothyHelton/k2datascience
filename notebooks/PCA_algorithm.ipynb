{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris.loc[:, 'species'] = (iris.loc[:, 'species']\n",
    "                          .astype('category'))\n",
    "                      \n",
    "iris.info()\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_formatter = {\n",
    "    'billions': FuncFormatter(lambda x, position: f'{x * 1e-9:.0f}'),\n",
    "    'millions': FuncFormatter(lambda x, position: f'{x * 1e-6:.0f}'),\n",
    "    'percent_convert': FuncFormatter(lambda x, position: f'{x * 100:.0f}%'),\n",
    "    'percent': FuncFormatter(lambda x, position: f'{x * 100:.0f}%'),\n",
    "    'thousands': FuncFormatter(lambda x, position: f'{x * 1e-3:.0f}'),\n",
    "}\n",
    "\n",
    "names = (\n",
    "    'Sepal Length',\n",
    "    'Sepal Width',\n",
    "    'Petal Length',\n",
    "    'Petal Width',\n",
    ")\n",
    "\n",
    "column_names = [x.replace(' ', '_').lower()\n",
    "                for x in names]\n",
    "\n",
    "size = {\n",
    "    'label': 14,\n",
    "    'legend': 12,\n",
    "    'title': 20,\n",
    "    'super_title': 24,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Principal Components Analysis\n",
    "\n",
    "Often, the desired goal is to reduce the dimensions of a **d**-dimensional dataset by projecting it onto a (**k**)-dimensional subspace (where **k** < **d**) in order to increase the computational efficiency while retaining most of the information. An important question is “what is the size of **k** that represents the data ‘well’?”\n",
    "\n",
    "Later, we will compute eigenvectors (the principal components) of a dataset and collect them in a projection matrix. Each of those eigenvectors is associated with an eigenvalue which can be interpreted as the “length” or “magnitude” of the corresponding eigenvector. If some eigenvalues have a significantly larger magnitude than others that the reduction of the dataset via PCA onto a smaller dimensional subspace by dropping the “less informative” eigenpairs is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 - Explore the Iris Data Set\n",
    "\n",
    "[Original Data](https://archive.ics.uci.edu/ml/datasets/Iris). [Background Info](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure('Iris Violin Plot',\n",
    "                 figsize=(12, 5), facecolor='white',\n",
    "                 edgecolor='black')\n",
    "rows, cols = (1, 2)\n",
    "ax0 = plt.subplot2grid((rows, cols), (0, 0))\n",
    "ax1 = plt.subplot2grid((rows, cols), (0, 1), sharey=ax0)\n",
    "\n",
    "sns.boxplot(data=iris, width=0.4, ax=ax0)\n",
    "sns.violinplot(data=iris, inner='quartile', ax=ax1)\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.set_xlabel('Characteristics', fontsize=size['label'])\n",
    "    ax.set_xticklabels(names)\n",
    "    ax.set_ylabel('Centimeters $(cm)$', fontsize=size['label'])\n",
    "\n",
    "plt.suptitle('Iris Dataset', fontsize=size['title']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure('Iris Data Distribution Plots', figsize=(10, 15),\n",
    "                 facecolor='white', edgecolor='black')\n",
    "rows, cols = (4, 2)\n",
    "ax0 = plt.subplot2grid((rows, cols), (0, 0))\n",
    "ax1 = plt.subplot2grid((rows, cols), (0, 1))\n",
    "ax2 = plt.subplot2grid((rows, cols), (1, 0))\n",
    "ax3 = plt.subplot2grid((rows, cols), (1, 1))\n",
    "ax4 = plt.subplot2grid((rows, cols), (2, 0))\n",
    "ax5 = plt.subplot2grid((rows, cols), (2, 1))\n",
    "ax6 = plt.subplot2grid((rows, cols), (3, 0))\n",
    "ax7 = plt.subplot2grid((rows, cols), (3, 1))\n",
    "\n",
    "n_bins = 40\n",
    "\n",
    "for n, ax, data in zip(range(4), (ax0, ax2, ax4, ax6), column_names):\n",
    "    iris[data].plot(kind='hist', alpha=0.5, bins=n_bins, color=f'C{n}',\n",
    "                    edgecolor='black', label='_nolegend_', ax=ax)\n",
    "    ax.axvline(iris[data].mean(), color='crimson', label='Mean',\n",
    "               linestyle='--')\n",
    "    ax.axvline(iris[data].median(), color='black', label='Median',\n",
    "               linestyle='-.')\n",
    "    ax.set_title(names[n])\n",
    "    ax.set_ylabel('Count', fontsize=size['label'])\n",
    "\n",
    "for n, ax, data in zip(range(4), (ax1, ax3, ax5, ax7), column_names):\n",
    "    sns.distplot(iris[data], axlabel=False, bins=n_bins,\n",
    "                 hist_kws={'alpha': 0.5, 'color': f'C{n}',\n",
    "                           'edgecolor': 'black'},\n",
    "                 kde_kws={'color': 'darkblue', 'label': 'KDE'},\n",
    "                 ax=ax)\n",
    "    ax.set_title(names[n])\n",
    "    ax.set_ylabel('Density', fontsize=size['label'])\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7):\n",
    "    ax.legend(fontsize=size['legend'])\n",
    "    ax.set_xlabel('Centimeters ($cm$)', fontsize=size['label'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Iris Data Distribution Plots',\n",
    "             fontsize=size['super_title'], y=1.03);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.pairplot(iris,\n",
    "                    diag_kws={'alpha': 0.5, 'bins': 30, 'edgecolor': 'black'},\n",
    "                    hue='species', markers=['o', 's', 'D'],\n",
    "                    plot_kws={'alpha': 0.7})\n",
    "\n",
    "grid.fig.suptitle('Iris Dataset Correlation',\n",
    "                  fontsize=size['super_title'], y=1.03)\n",
    "handles = grid._legend_data.values()\n",
    "labels = grid._legend_data.keys()\n",
    "grid._legend.remove()\n",
    "grid.fig.legend(bbox_to_anchor=(1.02, 0.5), fontsize=size['legend'],\n",
    "                handles=handles,\n",
    "                labels=[x.capitalize() for x in labels],\n",
    "                loc='center right')\n",
    "\n",
    "for n in range(4):\n",
    "    grid.axes[3, n].set_xlabel(names[n], fontsize=size['label'])\n",
    "    grid.axes[n, 0].set_ylabel(names[n], fontsize=size['label'])\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2 - Build a PCA Class\n",
    "\n",
    "General Steps for PCA ([walkthrough in R if you get stuck](http://alexhwoods.com/pca/)):\n",
    "1. Standardize the data.\n",
    "2. Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "3. Sort eigenvalues in descending order and choose the **k** eigenvectors that correspond to the **k** largest eigenvalues where **k** is the number of dimensions of the new feature subspace (**k ≤ d**).\n",
    "4. Construct the projection matrix **W** from the selected **k** eigenvectors.\n",
    "5. Transform the original dataset **X** via **W** to obtain a **k**-dimensional feature subspace **Y**.\n",
    "\n",
    "The class should be able to:\n",
    "- Calculate the principal components with an optional parameter\n",
    "- Project onto a 2-dimensional feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K2PCA:\n",
    "    \"\"\"\n",
    "    Class to perform a Principal Component Analysis.\n",
    "    \n",
    "    :Attributes:\n",
    "    \n",
    "    - **categories**: *pd.Series* categories of data\n",
    "    - **covariance**: *np.array* covariance matrix of normalized data\n",
    "    - **data**: *pd.DataFrame* original data\n",
    "    - **eigen_val**: *np.array* covariance matrix eigenvalues\n",
    "    - **eigen_vec**: *np.array* covariance matrix eigenvectors\n",
    "    - **n_components**: *int* number of priciple components to return\n",
    "    - **normalize**: *np.array* normalized data\n",
    "    - **trans_data**: *pd.DataFrame* original data transformed into the \\\n",
    "        two dimensional component space\n",
    "    - **variance**: *np.array* percentage of variance by feature\n",
    "    \"\"\"\n",
    "    def __init__(self, data, categories, n_components=None):\n",
    "        self.categories = categories\n",
    "        self.data = data\n",
    "        self.eigen_val = None\n",
    "        self.eigen_vec = None\n",
    "        self.n_components = n_components\n",
    "        self.trans_data =None\n",
    "        self._covariance = None\n",
    "        self._normalize = None\n",
    "        self._variance = None\n",
    "    \n",
    "    @property\n",
    "    def covariance(self):\n",
    "        self.calc_normalize()\n",
    "        self.calc_covariance()\n",
    "        return self._covariance\n",
    "    \n",
    "    @property\n",
    "    def normalize(self):\n",
    "        self.calc_normalize()\n",
    "        return self._normalize\n",
    "    \n",
    "    @property\n",
    "    def variance(self):\n",
    "        self.calc_variance()\n",
    "        return self._variance\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'K2PCA(data={self.data}, n_components{self.n_components})'\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Standardize the data and determine the sorted eigenvalues and eigenvectors.\n",
    "        \"\"\"\n",
    "        self.calc_normalize()\n",
    "        self.calc_covariance()\n",
    "        self.calc_eigen()\n",
    "    \n",
    "    def calc_covariance(self):\n",
    "        \"\"\"\n",
    "        Calculate the covariance matrix.\n",
    "        \"\"\"\n",
    "        self._covariance = np.cov(self._normalize, rowvar=False)\n",
    "    \n",
    "    def calc_eigen(self):\n",
    "        \"\"\"\n",
    "        Calculate the covariance eigenvalues and eigenvectors.\n",
    "        \n",
    "        .. note:: NumPy eig returns the eigen vector as a column\n",
    "        \"\"\"\n",
    "        if self._covariance is None:\n",
    "            self.calc_covariance()\n",
    "            \n",
    "        self.eigen_val, self.eigen_vec = (np.linalg\n",
    "                                          .eig(self._covariance))\n",
    "        idx = self.eigen_val.argsort()[::-1]   \n",
    "        self.eigen_val = self.eigen_val[idx]\n",
    "        self.eigen_vec = self.eigen_vec[:,idx].T\n",
    "        \n",
    "    def calc_normalize(self, data=None):\n",
    "        \"\"\"\n",
    "        Standardize the data.\n",
    "        \n",
    "        :param pd.DataFrame data: data to be normalized\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            \n",
    "        self._normalize = (sklearn.preprocessing\n",
    "                           .StandardScaler()\n",
    "                           .fit_transform(data))\n",
    "        \n",
    "    def calc_variance(self):\n",
    "        \"\"\"\n",
    "        Calculate the percentage of variance by feature.\n",
    "        \"\"\"\n",
    "        if self.eigen_val is None:\n",
    "            self.calc_eigen()\n",
    "            \n",
    "        self._variance = self.eigen_val / pca.eigen_val.sum()\n",
    "    \n",
    "    def filter_components(self):\n",
    "        \"\"\"\n",
    "        Return the first n components specified by n_components attribute.\n",
    "        \"\"\"\n",
    "        if self.eigen_val is None:\n",
    "            self.calc_eigen()\n",
    "            \n",
    "        if self.n_components is not None:\n",
    "            self.eigen_val = self.eigen_val[idx][:self.n_components]\n",
    "            self.eigen_vec = self.eigen_vect[idx][:self.n_components]\n",
    "    \n",
    "    def plot_variance(self, save=False):\n",
    "        \"\"\"\n",
    "        Plot the feature percentage of variance per component.\n",
    "        \n",
    "        :param bool save: if True the figure will be saved\n",
    "        \"\"\"\n",
    "        if self._variance is None:\n",
    "            self.calc_variance()\n",
    "            \n",
    "        var_pct = pd.Series(self._variance)\n",
    "        cum_var_pct = var_pct.cumsum()\n",
    "        ax = (pd.concat([var_pct, cum_var_pct], axis=1)\n",
    "              .rename(index={x: x + 1 for x in range(var_pct.size)})\n",
    "              .plot(kind='bar', alpha=0.5, edgecolor='black',\n",
    "                    figsize=(10, 5)))\n",
    "        \n",
    "        ax.set_title('Dataset Components', fontsize=size['title'])\n",
    "        ax.legend(['Individual Variance', 'Cumulative Variance'],\n",
    "                  fontsize=size['legend'])\n",
    "        ax.set_xlabel('Principal Components',\n",
    "                      fontsize=size['label'])\n",
    "        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "        ax.set_ylabel('Percent (%)', fontsize=size['label'])\n",
    "        ax.yaxis.set_major_formatter(ax_formatter['percent'])\n",
    "\n",
    "        for patch in ax.patches:\n",
    "            height = patch.get_height()\n",
    "            ax.text(x=patch.get_x() + patch.get_width() / 2,\n",
    "                    y=height + 0.01,\n",
    "                    s=f'{height * 100:1.1f}%',\n",
    "                    ha='center')\n",
    "    \n",
    "        if save:\n",
    "            plt.savefig(f'variance_pct.png', bbox_inches='tight',\n",
    "                        bbox_extra_artists=[size['super_title']])\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_transform_2d(self):\n",
    "        \"\"\"\n",
    "        Plot the original data in the 2D PCA space.\n",
    "        \"\"\"\n",
    "        if self.trans_data is None:\n",
    "            self.transform_2d()\n",
    "            \n",
    "        grid = sns.lmplot(x='comp_1', y='comp_2', data=pca.trans_data,\n",
    "                          hue='categories', fit_reg=False,\n",
    "                          markers=['o', 's', 'd'], size=6)\n",
    "        \n",
    "        grid.fig.suptitle('Principal Components',\n",
    "                          fontsize=size['title'], y=1.05)\n",
    "        handles = grid._legend_data.values()\n",
    "        labels = grid._legend_data.keys()\n",
    "        grid._legend.remove()\n",
    "        grid.fig.legend(bbox_to_anchor=(1.02, 0.94),\n",
    "                        fontsize=size['legend'], handles=handles,\n",
    "                        labels=[x.capitalize() for x in labels],\n",
    "                        loc='center right')\n",
    "        grid.axes[0, 0].set_xlabel('PCA $1^{st}$ Component',\n",
    "                                   fontsize=size['label'])\n",
    "        grid.axes[0, 0].set_ylabel('PCA $2^{nd}$ Component',\n",
    "                                   fontsize=size['label'])\n",
    "\n",
    "    def transform_2d(self):\n",
    "        \"\"\"\n",
    "        Transform the original data into the 2D PCA space.\n",
    "        \"\"\"\n",
    "        if self.eigen_vec is None:\n",
    "            self.calc_eigen()\n",
    "            \n",
    "        trans = self.eigen_vec[:2].T\n",
    "        self.trans_data = (pd.DataFrame(self._normalize.dot(trans),\n",
    "                                        columns=['comp_1', 'comp_2'])\n",
    "                           .assign(categories=self.categories))\n",
    "        self.trans_data.loc[:, 'comp_2'] = self.trans_data.comp_2 * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3 - Try it out on the Iris Data Set\n",
    "\n",
    "1. Plot the individual explained variance vs. cumulative explained variance.\n",
    "2. Plot the Iris data set on the new 2-dimensional feature subspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = K2PCA(iris.drop('species', axis=1), iris.species)\n",
    "pca.fit()\n",
    "pca.plot_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.plot_transform_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 4 - Check via Scikit-Learn\n",
    "\n",
    "This exercise was purely academic. You will always use an optimized version of PCA in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca.fit(sklearn.preprocessing\n",
    "        .StandardScaler()\n",
    "        .fit_transform(iris.drop('species', axis=1)))\n",
    "var_pct = pd.Series(pca.explained_variance_ratio_)\n",
    "cum_var_pct = var_pct.cumsum()\n",
    "ax = (pd.concat([var_pct, cum_var_pct], axis=1)\n",
    "      .plot(kind='bar', alpha=0.5, edgecolor='black',\n",
    "            figsize=(10, 5)))\n",
    "\n",
    "ax.set_title('Iris Data Components', fontsize=size['title'])\n",
    "ax.legend(['Individual Variance', 'Cumulative Variance'],\n",
    "          fontsize=size['legend'])\n",
    "ax.set_xlabel('Components', fontsize=size['label'])\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "ax.set_ylabel('Percent (%)', fontsize=size['label'])\n",
    "ax.yaxis.set_major_formatter(ax_formatter['percent'])\n",
    "\n",
    "for patch in ax.patches:\n",
    "    height = patch.get_height()\n",
    "    ax.text(x=patch.get_x() + patch.get_width() / 2,\n",
    "            y=height + 0.01,\n",
    "            s=f'{height * 100:1.1f}%',\n",
    "            ha='center')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The main component of the Iris data is the Sepal Length, which captures 92.5% of dataset variance.  One would be justified in removing all the other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 2\n",
    "pca = PCA(n_components=components)\n",
    "X = iris.drop('species', axis=1)\n",
    "X_std = (sklearn.preprocessing\n",
    "         .StandardScaler()\n",
    "         .fit_transform(X))\n",
    "y = pca.fit_transform(X_std)\n",
    "y = (pd.DataFrame(y, columns=['first', 'second'])\n",
    "     .assign(species=iris.species))\n",
    "\n",
    "grid = sns.lmplot(x='first', y='second', data=y,\n",
    "                  hue='species', fit_reg=False,\n",
    "                  markers=['o', 's', 'd'], size=6)\n",
    "grid.fig.suptitle('Principal Components',\n",
    "                  fontsize=size['title'], y=1.05)\n",
    "handles = grid._legend_data.values()\n",
    "labels = grid._legend_data.keys()\n",
    "grid._legend.remove()\n",
    "grid.fig.legend(bbox_to_anchor=(1.02, 0.94), fontsize=size['legend'],\n",
    "                handles=handles,\n",
    "                labels=[x.capitalize() for x in labels],\n",
    "                loc='center right')\n",
    "grid.axes[0, 0].set_xlabel('PCA $1^{st}$ Component',\n",
    "                           fontsize=size['label'])\n",
    "grid.axes[0, 0].set_ylabel('PCA $2^{nd}$ Component',\n",
    "                           fontsize=size['label'])\n",
    "\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
