{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Principal Components Analysis\n",
    "Timothy Helton\n",
    "\n",
    "---\n",
    "<br>\n",
    "<font color=\"red\">\n",
    "    NOTE:\n",
    "    <br>\n",
    "    This notebook uses code found in the\n",
    "    <a href=\"https://github.com/TimothyHelton/k2datascience/blob/master/k2datascience/pca.py\">\n",
    "    <strong>k2datascience.pca</strong></a> module.\n",
    "    To execute all the cells do one of the following items:\n",
    "    <ul>\n",
    "        <li>Install the k2datascience package to the active Python interpreter.</li>\n",
    "        <li>Add k2datascience/k2datascience to the PYTHON_PATH system variable.</li>\n",
    "        <li>Create a link to the pca.py file in the same directory as this notebook.</li>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k2datascience import pca\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 1 - Crowdedness at the Campus Gym\n",
    "\n",
    "The dataset consists of 26,000 people counts (about every 10 minutes) over the last year. In addition, I gathered extra info including weather and semester-specific information that might affect how crowded it is. The label is the number of people, which I'd like to predict given some subset of the features.\n",
    "\n",
    "Label:\n",
    "\n",
    "- Number of people\n",
    "\n",
    "Features:\n",
    "\n",
    "- timestamp (int; number of seconds since beginning of day)\n",
    "- day_of_week (int; 0 - 6)\n",
    "- is_weekend (int; 0 or 1)\n",
    "- is_holiday (int; 0 or 1)\n",
    "- apparent_temperature (float; degrees fahrenheit)\n",
    "- temperature (float; degrees fahrenheit)\n",
    "- is_start_of_semester (int; 0 or 1)\n",
    "\n",
    "[Based off the Kaggle dataset](https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** - We are going to apply Principal Component Analysis on the given dataset using scikit-learn (bonus points if you use your own optimized Python version). We want to find the components with the maximum variance. Features with little or no variance are dropped and then the data is trained on transformed dataset to apply machine learning models.\n",
    "\n",
    "1. Read in the gym dataset.\n",
    "2. Explore the data, the summay statistics and identify any strong positive or negative correlations between the features.\n",
    "3. Convert temperature and apparent temperature from Fahrenheit to Celcius.\n",
    "4. Extract the features to a new dataframe. The column you would eventually predict is `number_people`.\n",
    "5. Make a heatmap of the correlation.\n",
    "6. Run PCA on the feature dataframe, and plot the explained variance ratio of the principal components.\n",
    "7. Which components would you drop and why?\n",
    "8. Re-run PCA on the feature dataframe, restricting it to the number of principal components you want and plot the explained variance ratios again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = pca.Gym(label_column='people')\n",
    "gym.data_name = 'Gym'\n",
    "gym.feature_columns = [\n",
    "    'day_number',\n",
    "    'weekend',\n",
    "    'holiday',\n",
    "    'apparent_temp',\n",
    "    'temp',\n",
    "    'start_of_semester',\n",
    "    'seconds',\n",
    "]\n",
    "\n",
    "header = '#' * 25\n",
    "print(f'\\n\\n{header}\\n### Data Head\\n{header}')\n",
    "gym.data.head()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Data Overview\\n{header}')\n",
    "gym.data.info()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Summary Statistics\\n{header}')\n",
    "gym.data.describe()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Absolute Correlation\\n{header}')\n",
    "(gym.data.corr()\n",
    " .people\n",
    " .abs()\n",
    " .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.plot_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.plot_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- The two temperature variables show a week correlation to correlation to number of people in the gym.\n",
    "- The following variables show minimal correlation to number of people in the gym.\n",
    "    - day_number\n",
    "    - weekend\n",
    "    - start_of_semester\n",
    "- The holiday variable shows no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.plot_variance(fig_size=(14,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.scree_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- From the PCA analysis the last two principle componenets will be neglected.\n",
    "    - For initial investigations neglecting the last three priniciple components would be justifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.n_components = 5\n",
    "gym.calc_components()\n",
    "gym.plot_variance()\n",
    "gym.scree_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - IMDB Movie Data\n",
    "\n",
    "How can we tell the greatness of a movie before it is released in cinema?\n",
    "\n",
    "This question puzzled me for a long time since there is no universal way to claim the goodness of movies. Many people rely on critics to gauge the quality of a film, while others use their instincts. But it takes the time to obtain a reasonable amount of critics review after a movie is released. And human instinct sometimes is unreliable.\n",
    "\n",
    "To answer this question, I scraped 5000+ movies from IMDB website using a Python library called \"scrapy\".\n",
    "\n",
    "The scraping process took 2 hours to finish. In the end, I was able to obtain all needed 28 variables for 5043 movies and 4906 posters (998MB), spanning across 100 years in 66 countries. There are 2399 unique director names, and thousands of actors/actresses. Below are the 28 variables:\n",
    "\n",
    "\"movie_title\" \"color\" \"num_critic_for_reviews\" \"movie_facebook_likes\" \"duration\" \"director_name\" \"director_facebook_likes\" \"actor_3_name\" \"actor_3_facebook_likes\" \"actor_2_name\" \"actor_2_facebook_likes\" \"actor_1_name\" \"actor_1_facebook_likes\" \"gross\" \"genres\" \"num_voted_users\" \"cast_total_facebook_likes\" \"facenumber_in_poster\" \"plot_keywords\" \"movie_imdb_link\" \"num_user_for_reviews\" \"language\" \"country\" \"content_rating\" \"budget\" \"title_year\" \"imdb_score\" \"aspect_ratio\"\n",
    "\n",
    "\n",
    "[Based off the Kaggle dataset](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** - We are going to apply Principal Component Analysis on the given dataset using scikit-learn (bonus points if you use your own optimized Python version). We want to find the components with the maximum variance. Features with little or no variance are dropped and then the data is trained on transformed dataset to apply machine learning models.\n",
    "\n",
    "1. Read in the movie dataset.\n",
    "2. Explore the data, the summay statistics and identify any strong positive or negative correlations between the features.\n",
    "3. Some columns contain numbers, while others contain words. Do some filtering to extract only the numbered columns and not the ones with words into a new dataframe.\n",
    "4. Remove null values and standardize the values.\n",
    "5. Create hexbin visualizations to get a feel for how the correlations between different features compare to one another. Can you draw any conclusions about the features?\n",
    "6. Create a heatmap of the pearson correlation of movie features. Detail your observations.\n",
    "7. Perform PCA on the dataset, and plot the individual and cumulative explained variance superimposed on the same graph.\n",
    "8. How many components do you want to use? Implement PCA and transform the dataset.\n",
    "9. Create a 2D and 3D scatter plot of the the 1st 2 and the 1st 3 components.\n",
    "10. Do you notice any distinct clusters in the plots? (*For future clustering assignment*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pca.Movies(label_column='imdb_score')\n",
    "movie.data_name = 'Movie'\n",
    "movie.feature_columns = movie.data_numeric.columns\n",
    "\n",
    "header = '#' * 25\n",
    "print(f'\\n\\n{header}\\n### Data Head\\n{header}')\n",
    "movie.data.head()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Data Overview\\n{header}')\n",
    "movie.data.info()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Summary Statistics\\n{header}')\n",
    "movie.data.describe()\n",
    "\n",
    "print(f'\\n\\n{header}\\n### Absolute Correlation\\n{header}')\n",
    "(movie.data.corr()\n",
    " .imdb_score\n",
    " .abs()\n",
    " .sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.top_correlation_joint_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- From the top four correlation joint plots it appears that the IMBD score does not have any dominate drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.plot_correlation_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.plot_variance(fig_size=(14,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.scree_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- For this dataset the first ten principle components.\n",
    "    - This will capture just shy of 90% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.n_components = 10\n",
    "movie.calc_components()\n",
    "movie.plot_variance()\n",
    "movie.scree_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.plot_component_2_vs_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.plot_componets_1_2_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "- Possible clusters appear to exist for movies with low and high IMDB sores.\n",
    "- The middle region of the data does not exhibit an identifiable pattern."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
